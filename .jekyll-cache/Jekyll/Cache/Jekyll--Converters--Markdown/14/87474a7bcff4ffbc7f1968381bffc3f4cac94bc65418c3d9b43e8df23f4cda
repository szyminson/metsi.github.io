I"¤Ó<p>Tematem na dzisiaj bÄ™dzie implementacja zespoÅ‚u klasyfikatorÃ³w. Jest to taki szczegÃ³lny rodzaj klasyfikatora, ktÃ³ry skÅ‚ada siÄ™Â z wielu klasyfikatorÃ³w. Zatem przejdÅºmy od razu do konkretÃ³w.</p>

<!--more-->

<h2 id="zespÃ³Å‚-klasyfikatorÃ³w">ZespÃ³Å‚ klasyfikatorÃ³w</h2>

<p>Na ostatnich zajÄ™ciach pokazane byÅ‚o jak napisaÄ‡ implementacjÄ™ klasyfikatora zgodnego z <code class="highlighter-rouge">API</code> <code class="highlighter-rouge">scikit-learn</code>. Dzisiaj zajmiemy siÄ™ zespoÅ‚ami klasyfikatorÃ³w, inaczej nazywane komitetami klasyfikatorÃ³w.</p>

<h3 id="wstÄ™p">WstÄ™p</h3>

<p>Na poczÄ…tek trochÄ™ wyjaÅ›nieÅ„. GÅ‚Ã³wna idea budowania zespoÅ‚Ã³w klasyfikatorÃ³w oparta jest na tym, Å¼e najpierw wiele modeli jest uczonych, a nastÄ™pnie ich predykcje sÄ… Å‚Ä…czone w celu podjÄ™cia wspÃ³lnej decyzji. Zazwyczaj modele te budowane sÄ… w oparciu o â€œprosteâ€ klasyfikatory. WspÃ³lne podejmowanie decyzji pozwala uzyskiwanie dobrej jakoÅ›ci klasyfikacji przy zachowaniu odpowiedniej generalizacji. KluczowÄ… tutaj sprawÄ… jest zapewnienie dywersyfikacji komitetu lub inaczej mÃ³wiÄ…c rÃ³Å¼norodnoÅ›ci modeli za pomocÄ… odpowiednich technik. Bez sensowne jest tworzenie zespoÅ‚u skÅ‚adajÄ…cego siÄ™ z klasyfikatorÃ³w, ktÃ³re podejmujÄ… identyczne decyzje. Dlatego stosuje siÄ™ odpowiednie metody zapewniajÄ…ce zachowanie rÃ³Å¼norodnoÅ›ci.</p>

<p>NastÄ™pnie majÄ…c juÅ¼ taki zespÃ³Å‚ musimy w pewien sposÃ³b agregowaÄ‡ odpowiedzi wszystkich modeli w jednÄ… wspÃ³lnÄ… odpowiedÅº caÅ‚ego komitetu. To w jaki sposÃ³b jest to wykonywane okreÅ›la reguÅ‚a decyzyjna. IstniejÄ… rÃ³Å¼ne sposoby, jednak najbardziej znanym jest gÅ‚osowanie twarde. KaÅ¼dy model wskazuje jakÄ…Å› etykietÄ™, a wiÄ™kszoÅ›Ä‡ â€œgÅ‚osÃ³wâ€ decyduje o tym jaka jest podejmowana decyzja.  Jest to doÅ›Ä‡ podobna zasada do koÅ‚a ratunkowego â€œPytanie do publicznoÅ›ciâ€ w popularnym teleturnieju â€œMilionerzyâ€. ZakÅ‚adamy, Å¼e wszyscy na publicznoÅ›ci majÄ… rÃ³wnÄ… wagÄ™ gÅ‚osu i kaÅ¼dy typuje jakÄ…Å› odpowiedÅº. Wartym wspomnienia jest to, Å¼e osoby tam siÄ™ znajdujÄ…ce zazwyczaj nie sÄ… ekspertami w dziedzinie danego pytania, jednak wspÃ³lnymi siÅ‚ami czÄ™sto potrafiÄ… wskazaÄ‡ poprawnÄ… odpowiedÅº na zadane pytanie.</p>

<p>ZaÅ‚Ã³Å¼my teraz, Å¼e mamy pewien problem dwuwymiarowy, ktÃ³ry prÃ³bujemy rozwiÄ…zaÄ‡ za pomocÄ… liniowego klasyfikatora. Jak to jest widoczne na rysunkach poniÅ¼ej wyznaczenie linii rozdzielajÄ…cej idealnie dane jest niemoÅ¼liwe, poniewaÅ¼ mamy tutaj problem liniowo nieseparowalny. Niebieskie kropki to jedna klasa, a czerwona to druga.</p>

<p><img src="/examples/kod7/figures/Ensemble0.png" alt="Ensemble0" style="zoom: 67%;" />  <img src="/examples/kod7/figures/Ensemble1.png" alt="Ensemble1" style="zoom: 67%;" />
 <img src="/examples/kod7/figures/Ensemble2.png" alt="Ensemble2" style="zoom: 67%;" />  <img src="/examples/kod7/figures/Ensemble3.png" alt="Ensemble2" style="zoom: 67%;" /></p>

<p>Pojawia siÄ™ jednak pewne â€œaleâ€. MoÅ¼emy ten problem w miarÄ™ skutecznie rozwiÄ…zaÄ‡ za pomocÄ… liniowych klasyfikatorÃ³w, ale tworzÄ…c z nich komitet klasyfikatorÃ³w. Kiedy poÅ‚Ä…czymy w odpowiedni sposÃ³b trzy modele przedstawione na rysunkach powyÅ¼ej, otrzymamy zespÃ³Å‚ klasyfikatorÃ³w. Tak wyuczony komitet jest w stanie wyznaczyÄ‡ granicÄ™ decyzyjnÄ… opartÄ… na modelach linowych dla problemu liniowo nieseparowalnego. Jak widaÄ‡ na poniÅ¼szym rysunku jakoÅ›Ä‡ predykcji liniowych modeli roÅ›nie, gdy zaczynajÄ… dziaÅ‚aÄ‡ w jednym zespole. Nie przerywana linia oznacza granicÄ™ decyzji dla caÅ‚ego zespoÅ‚u.</p>

<p><img src="/examples/kod7/figures/EnsembleFull.png" alt="EnsembleFull" style="zoom:80%;" /></p>

<h3 id="implementacja">Implementacja</h3>

<p>Teraz znajÄ…c juÅ¼ pewne podstawy, moÅ¼emy przejÅ›Ä‡ do czÄ™Å›ci implementacyjnej. Stworzenie zespoÅ‚u to oznacza uczenie wielu modeli, ktÃ³re podejmujÄ… wspÃ³lnÄ…Â decyzjÄ™. Musimy pamiÄ™taÄ‡ o tym, Å¼e komitet klasyfikatorÃ³w wymaga od nas zapewnienia dywersyfikacji. W tym celu wykorzystamy do tego doÅ›Ä‡ znanÄ… technikÄ™ tworzenia losowych podprzestrzeni cech (ang. Random Subspace Ensemble). Oznacza to, Å¼e kaÅ¼dy model bÄ™dzie siÄ™ uczyÄ‡ oraz dokonywaÄ‡ predykcji wykorzystujÄ…c niepeÅ‚ny i losowy zestaw cech danych.</p>

<h4 id="importy">Importy</h4>

<p>Na poczÄ…tek stwÃ³rzmy sobie nowy plik o nazwie <code class="highlighter-rouge">rse.py</code>. W samym nagÅ‚Ã³wku naszego pliku umieszczamy odpowiednie biblioteki i funkcje potrzebne nam w dalszej czÄ™Å›ci programu. CaÅ‚oÅ›Ä‡ zasadniczo siÄ™Â nie rÃ³Å¼ni jak podczas implementacji klasyfikatora. Jedyny wyjÄ…tek to zamiast <code class="highlighter-rouge">BaseEstimator</code>, bÄ™dziemy dziedziczyÄ‡ klasÄ™ <code class="highlighter-rouge">BaseEnsemble</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaseEnsemble</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">ClassifierMixin</span><span class="p">,</span> <span class="n">clone</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.validation</span> <span class="kn">import</span> <span class="n">check_array</span><span class="p">,</span> <span class="n">check_is_fitted</span><span class="p">,</span> <span class="n">check_X_y</span>
</code></pre></div></div>

<h4 id="klasa">Klasa</h4>

<p>Kolejny krok to definicja klasy oraz inicjalizator. Parametry jakie bÄ™dzie przyjmowaÄ‡ implementowany zespÃ³Å‚ to <code class="highlighter-rouge">base_estimator</code>, ktÃ³ra oznacza klasyfikator jaki bÄ™dzie uÅ¼ywany do tworzenia modeli w komitecie. Maksymalna liczba modeli jest wyraÅ¼ana za pomocÄ… parametru <code class="highlighter-rouge">n_estimators</code>, gdzie wartoÅ›Ä‡ domyÅ›lna jest rÃ³wna 10.  Parametr <code class="highlighter-rouge">n_subspace_features</code> wyraÅ¼a liczbÄ™ wykorzystanych cech do stworzenia jednej podprzestrzeni z wartoÅ›ciÄ…Â domyÅ›lnÄ… 5. IloÅ›Ä‡ tych podprzestrzeni jest rÃ³wna liczbie modeli. NastÄ™pnie znajduje siÄ™ parametr ` hard_voting<code class="highlighter-rouge"> okreÅ›lajÄ…cy jaki jest tryb podejmowania decyzji, domyÅ›lnie ustawiony na </code>True<code class="highlighter-rouge">. W dalszej czÄ™Å›ci znajdzie siÄ™ wiÄ™cej informacji do czego nam ten parametr posÅ‚uÅ¼y. Ostatni to dobrze znany parametr </code>random_state`, ktÃ³ry pozwoli na zachowanie pewnej powtarzalnoÅ›ci podczas wykonywania eksperymentÃ³w.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RandomSubspaceEnsemble</span><span class="p">(</span><span class="n">BaseEnsemble</span><span class="p">,</span> <span class="n">ClassifierMixin</span><span class="p">):</span>
    <span class="s">"""
    Random subspace ensemble
    Komitet klasyfikatorow losowych podprzestrzeniach cech
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_estimator</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_subspace_features</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">hard_voting</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Klasyfikator bazowy
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span> <span class="o">=</span> <span class="n">base_estimator</span>
        <span class="c1"># Liczba klasyfikatorow
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="n">n_estimators</span>
        <span class="c1"># Liczba cech w jednej podprzestrzeni
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">n_subspace_features</span> <span class="o">=</span> <span class="n">n_subspace_features</span>
        <span class="c1"># Tryb podejmowania decyzji
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">hard_voting</span> <span class="o">=</span> <span class="n">hard_voting</span>
        <span class="c1"># Ustawianie ziarna losowosci
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="uczenie">Uczenie</h4>

<p>Teraz pora na funkcjÄ™, ktÃ³ra pozwoli na uczenie caÅ‚ego zespoÅ‚u. W pierwszej kolejnoÅ›ci dokonujemy pewnych rutynowych krokÃ³w. Sprawdzenie poprawnoÅ›ci danych, zapis nazw klas, dodatkowo zapis liczby atrybutÃ³w. NastÄ™pnie musimy sprawdziÄ‡ czy liczba cech, ktÃ³re chcemy wykorzystaÄ‡ do tworzenia podprzestrzeni nie jest wiÄ™ksza od liczby cech jakie posiadajÄ… dane.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># Sprawdzenie czy X i y maja wlasciwy ksztalt
</span>        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="c1"># Przehowywanie nazw klas
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="c1"># Zapis liczby atrybutow
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Czy liczba cech w podprzestrzeni jest mniejsza od calkowitej liczby cech
</span>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_subspace_features</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Number of features in subspace higher than number of features."</span><span class="p">)</span>
</code></pre></div></div>
<p>Dopiero teraz przechodzimy do wÅ‚aÅ›ciwego dziaÅ‚ania. Losujemy numery indeksÃ³w podprzestrzeni cech dla modeli, ktÃ³re bÄ™dÄ… tworzyÄ‡ projektowany komitet. Indeksy te musimy zapisaÄ‡ w zmiennej <code class="highlighter-rouge">self.subspaces</code>, poniewaÅ¼ bÄ™dÄ… pÃ³Åºniej potrzebne podczas predykcji. NastÄ™pnie tworzymy pusty komitet <code class="highlighter-rouge">self.ensembles_</code> i wypeÅ‚niamy go modelami wyuczonymi na wczeÅ›niej wylosowanych podprzestrzeniach cech. Teraz istotny krok, to uÅ¼ycie do tego metody <code class="highlighter-rouge">clone()</code>. Jest to bardzo waÅ¼ne, aby tworzÄ…c nowe modele â€œklonowaÄ‡â€ je. MoÅ¼e siÄ™ zdarzyÄ‡ sytuacja taka, Å¼e bez uÅ¼ycia tej funkcji zespÃ³Å‚ bÄ™dzie skÅ‚adaÄ‡ siÄ™ z wielu, ale niestety identycznych modeli.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="c1"># Wylosowanie podprzestrzeni cech
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">subspaces</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_subspace_features</span><span class="p">))</span>

        <span class="c1"># Wyuczenie nowych modeli i stworzenie zespolu
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">ensemble_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ensemble_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">subspaces</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">y</span><span class="p">))</span>

        <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></div>

<h4 id="predykcja">Predykcja</h4>

<p>Podobnie jak na poczÄ…tku uczenia musimy dokonaÄ‡ pewnych kontroli. Po pierwsze sprawdzamy czy nasz komitet jest wyuczony. Takie zabezpieczenie uchroni przed nierozwaÅ¼nymi uÅ¼ytkownikami naszej implementacji, ktÃ³rzy zapragnÄ… coÅ› sklasyfikowaÄ‡ bez wczeÅ›niejszego wyuczenia metody. NastÄ™pnie kontrola poprawnoÅ›ci danych i zgodnoÅ›ci liczby cech z danymi uÅ¼ytych do uczenia.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># Sprawdzenie czy modele sa wyuczone
</span>        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">"classes_"</span><span class="p">)</span>
        <span class="c1"># Sprawdzenie poprawnosci danych
</span>        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="c1"># Sprawdzenie czy liczba cech siÄ™ zgadza
</span>        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"number of features does not match"</span><span class="p">)</span>
</code></pre></div></div>
<p>MajÄ…c juÅ¼ pewnoÅ›Ä‡, Å¼e wszystko siÄ™ zgadza moÅ¼emy przystÄ…piÄ‡ do predykcji. W tym miejscu pojawia siÄ™ wspomniany wczeÅ›niej parametr <code class="highlighter-rouge">hard_voting</code>. Na samym wstÄ™pie zostaÅ‚ opisany sposÃ³b gÅ‚osowania twardego. Z wykorzystaniem takiej reguÅ‚y decyzyjnej kaÅ¼dy model wskazuje etykietÄ™ dla kaÅ¼dej prÃ³bki, a na podstawie wiÄ™kszoÅ›ci wskazaÅ„ podejmowana jest ostateczna predykcja caÅ‚ego zespoÅ‚u.</p>

<p>Implementacja przebiega w nastÄ™pujÄ…cy sposÃ³b. Na poczÄ…tku tworzona jest pusta lista <code class="highlighter-rouge">pred_</code>, ktÃ³re zostaje wypeÅ‚niona predykcjami poszczegÃ³lnych modeli. PamiÄ™tamy o tym, Å¼e dobieramy odpowiednie podprzestrzenie cech tak samo jak podczas uczenia. PÃ³Åºniej zamieniana ona jest na macierz typu <code class="highlighter-rouge">ndarray</code> z biblioteki <code class="highlighter-rouge">numpy</code>. NastÄ™pnie po dokonaniu transpozycji (<code class="highlighter-rouge">pred_.T</code>), predykcje lÄ…dujÄ… w funkcji pozwalajÄ…cej na podliczenie maksymalnej liczby etykiet dla poszczegÃ³lnych prÃ³bek. Na sam koniec zwracana jest ostateczna predykcja caÅ‚ego zespoÅ‚u.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hard_voting</span><span class="p">:</span>
            <span class="c1"># Podejmowanie decyzji na podstawie twardego glosowania
</span>            <span class="n">pred_</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="c1"># Modele w zespole dokonuja predykcji
</span>            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">member_clf</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ensemble_</span><span class="p">):</span>
                <span class="n">pred_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">member_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">subspaces</span><span class="p">[</span><span class="n">i</span><span class="p">]]))</span>
            <span class="c1"># Zamiana na miacierz numpy (ndarray)
</span>            <span class="n">pred_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pred_</span><span class="p">)</span>
            <span class="c1"># Liczenie glosow
</span>            <span class="n">prediction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">arr</span><span class="o">=</span><span class="n">pred_</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
            <span class="c1"># Zwrocenie predykcji calego zespolu
</span>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">prediction</span><span class="p">]</span>
</code></pre></div></div>
<p>DecyzjÄ™ moÅ¼na rÃ³wnieÅ¼ podejmowaÄ‡ za pomocÄ… gÅ‚osowania miÄ™kkiego. Jest to podejÅ›cie, ktÃ³re rÃ³wnieÅ¼ warto poznaÄ‡. Predykcja jest tutaj oparta na Å›redniej wartoÅ›ci wektorÃ³w wsparcia modeli tworzÄ…cych zespÃ³Å‚ klasyfikatorÃ³w. W pierwszym kroku trzeba pobraÄ‡ wspomniane wczeÅ›niej wektory wsparcia ze wszystkich modeli (funkcja <code class="highlighter-rouge">ensemble_support_matrix</code>). Tutaj rÃ³wnieÅ¼ musimy pamiÄ™taÄ‡ o tym, Å¼e dobieramy odpowiednie podprzestrzenie cech dla modeli. Estymacja wektorÃ³w wsparcia okreÅ›li nam prawdopodobieÅ„stwo przynaleÅ¼noÅ›ci klasyfikowanego obiektu do kaÅ¼dej istniejÄ…cej klasy. W odpowiedzi (<code class="highlighter-rouge">esm</code>) nie uzyskamy wiÄ™c macierzy etykiet, a macierz wartoÅ›ci zmiennoprzecinkowych. NastÄ™pnie za pomocÄ… funkcji <code class="highlighter-rouge">np.mean</code> obliczamy Å›rednie wsparcie. W kolejnym kroku wskazujemy, ktÃ³ra klasa posiada najwyÅ¼sze prawdopodobieÅ„stwo przynaleÅ¼noÅ›ci dla danego obiektu <code class="highlighter-rouge">np.argmax</code>, aby na koniec zwrÃ³ciÄ‡ uzyskane predykcje.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Podejmowanie decyzji na podstawie wektorow wsparcia
</span>            <span class="n">esm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ensemble_support_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="c1"># Wyliczenie sredniej wartosci wsparcia
</span>            <span class="n">average_support</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">esm</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="c1"># Wskazanie etykiet
</span>            <span class="n">prediction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">average_support</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># Zwrocenie predykcji calego zespolu
</span>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">prediction</span><span class="p">]</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">ensemble_support_matrix</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># Wyliczenie macierzy wsparcia
</span>        <span class="n">probas_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">member_clf</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ensemble_</span><span class="p">):</span>
            <span class="n">probas_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">member_clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">subspaces</span><span class="p">[</span><span class="n">i</span><span class="p">]]))</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">probas_</span><span class="p">)</span>
</code></pre></div></div>
<h2 id="proste-eksperymenty">Proste eksperymenty</h2>

<p>StworzyliÅ›my juÅ¼ zespÃ³Å‚ klasyfikatorÃ³w, wiÄ™c dobrze byÅ‚oby sprawdziÄ‡ czy ta implementacja w ogÃ³le dziaÅ‚a. Przeprowadzenie paru bardzo prostych eksperymentÃ³w pozwoli nam na szybkie sprawdzenie dziaÅ‚ania oraz obserwacjÄ™Â wpÅ‚ywu niektÃ³rych parametrÃ³w na jakoÅ›Ä‡ predykcji komitetu. PamiÄ™tajmy, Å¼e bÄ™dÄ… to raczej badania uproszczone, co nie pozwali na bezapelacyjne stwierdzenia i wnioski.</p>

<h3 id="przygotowanie">Przygotowanie</h3>

<p>Na poczÄ…tek importujemy odpowiednie â€œnarzÄ™dziaâ€, ktÃ³re pozwolÄ… na przeprowadzenie prostych testÃ³w naszej metody. Klasycznie biblioteka <code class="highlighter-rouge">numpy</code>, do tego nowa metoda <code class="highlighter-rouge">RandomSubspaceEnsemble</code>. PrzydaÅ‚by siÄ™ teÅ¼ jakiÅ› bazowy klasyfikator w tym wypadku uÅ¼yjemy <code class="highlighter-rouge">GaussianNB</code>. Aby dodaÄ‡ odrobinÄ™ rzetelnoÅ›ci naszym testom uÅ¼yjemy walidacji krzyÅ¼owej <code class="highlighter-rouge">RepeatedStratifiedKFold</code>, a komitet zmierzymy jak dokÅ‚adnie dziaÅ‚a za pomocÄ… <code class="highlighter-rouge">accuracy_score</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">rse</span> <span class="kn">import</span> <span class="n">RandomSubspaceEnsemble</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RepeatedStratifiedKFold</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
</code></pre></div></div>

<p>Wczytujemy wybrany zbiÃ³r danych. Tym razem stawiam na dane ze zbioru <code class="highlighter-rouge">ionosphere</code>, ale gorÄ…co zachÄ™cam do samodzielnych prÃ³b z innymi zbiorami. Jak juÅ¼ dane zostanÄ™ wczytane oraz podzielone na cechy i etykiety cennÄ… informacjÄ… bÄ™dzie liczba cech. W tym wypadku sÄ… to 34 rÃ³Å¼ne atrybuty.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dataset</span> <span class="o">=</span> <span class="s">'ionosphere'</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s">"datasets/</span><span class="si">%</span><span class="s">s.csv"</span> <span class="o">%</span> <span class="p">(</span><span class="n">dataset</span><span class="p">),</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">","</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</code></pre></div></div>

<p>Jak juÅ¼ dane zostanÄ™ wczytane oraz podzielone na cechy i etykiety cennÄ… informacjÄ… bÄ™dzie liczba cech. W tym wypadku sÄ… to 34 rÃ³Å¼ne atrybuty.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Total number of features"</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<p>NastÄ™pnie musimy przyszykowaÄ‡ sobie ustawienia do walidacji krzyÅ¼owej. W tym wypadku wystarczÄ… 10 powtÃ³rzeÅ„Â z podziaÅ‚em 5 czÄ™Å›ci. PamiÄ™tamy o ustawieniu <code class="highlighter-rouge">random_state</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_splits</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n_repeats</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">rskf</span> <span class="o">=</span> <span class="n">RepeatedStratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="n">n_repeats</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="eksperyment-pierwszy">Eksperyment pierwszy</h3>

<p>Na poczÄ…tek sprawdÅºmy jaki wpÅ‚yw na metodÄ™ posiada wybrany tryb podejmowania decyzji - twarde i miÄ™kkie gÅ‚osowanie. W tym celu stworzymy sobie dwa identyczne testy, gdzie jedynÄ… rÃ³Å¼nicÄ… bÄ™dzie parametr <code class="highlighter-rouge">hard_voting</code> raz ustawiony na wartoÅ›Ä‡Â <code class="highlighter-rouge">True</code>, a w drugim podejÅ›ciu na wartoÅ›Ä‡Â <code class="highlighter-rouge">False</code>. WiedzÄ…c, Å¼e posiadamy 34 cechy spokojnie moÅ¼emy pozostawiÄ‡ wartoÅ›Ä‡ 5 cech na jednÄ… podprzestrzeÅ„. Liczba estymatorÃ³w i jednoczeÅ›nie podprzestrzeni rÃ³wna 10. Obie te wartoÅ›ci sÄ… domyÅ›lne.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomSubspaceEnsemble</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">GaussianNB</span><span class="p">(),</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_subspace_features</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">hard_voting</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">rskf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">])</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Hard voting - accuracy score: </span><span class="si">%.3</span><span class="s">f (</span><span class="si">%.3</span><span class="s">f)"</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomSubspaceEnsemble</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">GaussianNB</span><span class="p">(),</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_subspace_features</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">hard_voting</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">rskf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">])</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Soft voting - accuracy score: </span><span class="si">%.3</span><span class="s">f (</span><span class="si">%.3</span><span class="s">f)"</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span>
</code></pre></div></div>

<p>Uzyskane wyniki wskazujÄ…, Å¼e Å›rednia dokÅ‚adnoÅ›Ä‡ predykcji podczas tego testu jest lepsza dla gÅ‚osowania miÄ™kkiego. GÅ‚osowanie twarde wykazuje mniejsze odchylenie standardowe.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Hard voting - accuracy score: 0.800 (0.045)
Soft voting - accuracy score: 0.823 (0.053)
</code></pre></div></div>

<h3 id="eksperyment-drugi">Eksperyment drugi</h3>

<p>Kolejna wÅ‚aÅ›ciwoÅ›Ä‡ jakÄ… moÅ¼na sprawdziÄ‡ to wpÅ‚yw liczby cech uÅ¼ytej do stworzenia jednej podprzestrzeni. W tym wypadku przetestowana zostanÄ… przetestowane trzy rÃ³Å¼ne wartoÅ›ci 10, 15 i 20 cech.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomSubspaceEnsemble</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">GaussianNB</span><span class="p">(),</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_subspace_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">hard_voting</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">rskf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">])</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"10/34 features - accuracy score: </span><span class="si">%.3</span><span class="s">f (</span><span class="si">%.3</span><span class="s">f)"</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomSubspaceEnsemble</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">GaussianNB</span><span class="p">(),</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_subspace_features</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">hard_voting</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">rskf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">])</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"15/34 features - accuracy score: </span><span class="si">%.3</span><span class="s">f (</span><span class="si">%.3</span><span class="s">f)"</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomSubspaceEnsemble</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">GaussianNB</span><span class="p">(),</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_subspace_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">hard_voting</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">rskf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">])</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"20/34 features - accuracy score: </span><span class="si">%.3</span><span class="s">f (</span><span class="si">%.3</span><span class="s">f)"</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span>
</code></pre></div></div>

<p>Wyniki pokazujÄ…, Å¼e najlepszy wynik uzyskuje liczba 15 cech.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>10/34 features - accuracy score: 0.830 (0.050)
15/34 features - accuracy score: 0.855 (0.045)
20/34 features - accuracy score: 0.851 (0.047)
</code></pre></div></div>

<h3 id="eksperyment-trzeci">Eksperyment trzeci</h3>

<p>Ostatni eksperyment ma na celu sprawdzenie doboru liczby klasyfikatorÃ³w, tworzÄ…cych jeden zespÃ³Å‚. W tym teÅ›cie zostaÅ‚y wybrane wartoÅ›ci 5, 10 i 15.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomSubspaceEnsemble</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">GaussianNB</span><span class="p">(),</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_subspace_features</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">hard_voting</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">rskf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">])</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"5 estimators - accuracy score: </span><span class="si">%.3</span><span class="s">f (</span><span class="si">%.3</span><span class="s">f)"</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomSubspaceEnsemble</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">GaussianNB</span><span class="p">(),</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_subspace_features</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">hard_voting</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">rskf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">])</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"10 estimators - accuracy score: </span><span class="si">%.3</span><span class="s">f (</span><span class="si">%.3</span><span class="s">f)"</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomSubspaceEnsemble</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">GaussianNB</span><span class="p">(),</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">n_subspace_features</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">hard_voting</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">rskf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">])</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"15 estimators - accuracy score: </span><span class="si">%.3</span><span class="s">f (</span><span class="si">%.3</span><span class="s">f)"</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span>
</code></pre></div></div>

<p>Wyniki pokazujÄ…, Å¼e najlepsze rezultaty metoda uzyskuje posiadajÄ…c 10 modeli tworzÄ…cych zespÃ³Å‚.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>5 estimators - accuracy score: 0.840 (0.051)
10 estimators - accuracy score: 0.855 (0.045)
15 estimators - accuracy score: 0.854 (0.041)
</code></pre></div></div>

<h3 id="wnioski">Wnioski</h3>

<p>Z przeprowadzonych badaÅ„ moÅ¼na wyciÄ…gnÄ…Ä‡ parÄ™ ciekawych wnioskÃ³w. Po pierwsze widaÄ‡, Å¼e zaprojektowany zespÃ³Å‚ posiada parametry, ktÃ³re znaczÄ…co wpÅ‚ywajÄ… na jakoÅ›Ä‡ dziaÅ‚ania. DobÃ³r odpowiednich wartoÅ›ci parametrÃ³w, moÅ¼e okazaÄ‡ siÄ™ kluczowy podczas wykorzystywania tej metody. Kolejny wniosek jaki siÄ™ nasuwa to, Å¼e nie zawsze wiÄ™cej oznacza lepiej. ZauwaÅ¼alne jest to, Å¼e wiÄ™ksza liczba estymatorÃ³w lub cech w podprzestrzeni nie zawsze daje polepszenie dokÅ‚adnoÅ›ci. Tutaj trzeba raczej szukaÄ‡ wartoÅ›ci wywaÅ¼onej, niekoniecznie bardzo duÅ¼ej. Nie byÅ‚o to sprawdzane, ale moÅ¼na siÄ™ domyÅ›laÄ‡, Å¼e wiÄ™ksza liczba estymatorÃ³w lub cech w podprzestrzeni moÅ¼e siÄ™ wiÄ…zaÄ‡ z wiÄ™kszym obciÄ…Å¼eniem procesora, a co za tym idzie wydÅ‚uÅ¼onym czasem dziaÅ‚ania. PamiÄ™tajmy jednak, Å¼e sÄ… to tylko pewne hipotezy, ktÃ³re wymagajÄ… szerszego przebadania w celu uzyskania potwierdzenia.</p>

<p>To by byÅ‚o na tyle dzisiaj. GorÄ…co namawiam do â€œzabawyâ€ z uÅ¼yciem zaprezentowanej tu implementacji zespoÅ‚u klasyfikatorÃ³w opartego na losowych podprzestrzeniach cech. MoÅ¼e jakiÅ› wÅ‚asny pomysÅ‚ na dywersyfikacjÄ™ zespoÅ‚u lub ciekawy test tego zespoÅ‚u. MoÅ¼e wyjdÄ… inne wyniki, moÅ¼e dla innych danych lub jakiÅ› wÅ‚asnych eksperymentÃ³w. MoÅ¼e nasunÄ…Å‚ siÄ™ jakieÅ› nowe wnioski, czy powstanÄ… modyfikacje tego kodu. Wszystkie osoby, ktÃ³rym uda siÄ™ coÅ› napisaÄ‡ lub zrobiÄ‡, ogromnie zachÄ™cam do podzielenia siÄ™ swoimi implementacjami lub wnioskami mailowo. Czekam z wielkÄ… niecierpliwoÅ›ciÄ…!</p>

<p>jakub.klikowski@pwr.edu.pl</p>
:ET