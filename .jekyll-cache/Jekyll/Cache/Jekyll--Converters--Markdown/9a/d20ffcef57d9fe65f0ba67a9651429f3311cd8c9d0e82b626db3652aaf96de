I"­x<p>MinÄ…Å‚ tydzieÅ„, wiÄ™c powracamy z przykÅ‚adami. O ile siÄ™ nie mylÄ™, to jest to czwarty zestaw od koÅ„ca i zajmiemy siÄ™ w nim implementacjÄ… prostego (chociaÅ¼ w sumie kto wie) klasyfikatora zgodnego z <code class="highlighter-rouge">API</code> <code class="highlighter-rouge">scikit-learn</code>.
<!--more--></p>

<h2 id="klasyfikator-najbliÅ¼szych-centroidÃ³w-ang-nearest-centroid-classifier-nc">Klasyfikator najbliÅ¼szych centroidÃ³w (ang. <em>Nearest Centroid Classifier</em> (NC))</h2>

<p>Å»eby nikogo bardzo nie przestraszyÄ‡, weÅºmiemy na tapet klasyfikator najbliÅ¼szych centroidÃ³w. Koncepcyjnie jest on wrÄ™cz banalny, a jego dziaÅ‚anie moÅ¼na przedstawiÄ‡ nastÄ™pujÄ…co:</p>

<ol>
  <li>Dzielimy dane treningowe na osobne klasy.</li>
  <li>Wyznaczamy centroid kaÅ¼dej klasy problemu.</li>
  <li>Dla kaÅ¼dej instancji z zestawu testowego, obliczamy jej odlegÅ‚oÅ›Ä‡ (wedle zadanej wczeÅ›niej miary) do kaÅ¼dego z centroidÃ³w.</li>
  <li>PrzyporzÄ…dkowujemy klasyfikowanÄ… instancjÄ™ do tej klasy, ktÃ³rej centroid znajduje siÄ™ najbliÅ¼ej.</li>
</ol>

<h3 id="maÅ‚a-modyfikacja-zawierajÄ…ca-element-optymalizacji">MaÅ‚a modyfikacja zawierajÄ…ca element optymalizacji</h3>

<p>Å»eby jednak w dzisiejszym przykÅ‚adzie pojawiÅ‚o siÄ™ jakieÅ› wyzwanie, to do powyÅ¼szego modelu dodamy opcjonalnÄ… optymalizacjÄ™. Optymalizacja ta bÄ™dzie polegaÅ‚a na wykorzystaniu <em>reguÅ‚y trzech sigm</em> (znanej z wykÅ‚adu) do pozbycia siÄ™ z kaÅ¼dej z klas obserwacji, ktÃ³re nie mieszczÄ… siÄ™ w granicy 3 odchyleÅ„ standardowych od Å›redniej. Po kaÅ¼dej redukcji bÄ™dziemy ponownie wyznaczaÄ‡ centroid klasy i kolejny raz sprawdzaÄ‡, czy znajdujÄ… siÄ™ w niej obserwacje odstajÄ…ce (ang. <em>outliers</em>) â€“ proces bÄ™dzie powtarzany tak dÅ‚ugo, aÅ¼ wszystkie instancje klasy nie bÄ™dÄ… przekraczaÄ‡ rzeczonej granicy.</p>

<p>ProcedurÄ™ tÄ… moÅ¼emy przedstawiÄ‡ w nastÄ™pujÄ…cych krokach dla kaÅ¼dej z klas:</p>

<ol>
  <li>Wyznaczamy centroid klasy.</li>
  <li>Wyznaczamy odchylenie standardowe wszystkich instancji klasy.</li>
  <li>Wyznaczamy punkt graniczny w przestrzeni cech i odlegÅ‚oÅ›Ä‡ do granicy.</li>
  <li>Uznajemy za obserwacje odstajÄ…ce wszystkie te, ktÃ³re znajdujÄ… siÄ™ za wyznaczonÄ… granicÄ… i pozbywamy siÄ™ ich.</li>
  <li>Wracamy do punktu 1. i powtarzamy caÅ‚y proces tak dÅ‚ugo, dopÃ³ki bÄ™dziemy w stanie wykryÄ‡ kolejne <em>outliery</em>.</li>
</ol>

<p>Ale starczy juÅ¼ tej teorii, zabierzmy siÄ™ za kod.</p>

<h2 id="implementacja-wÅ‚asnego-estymatora-scikit-learn">Implementacja wÅ‚asnego estymatora <code class="highlighter-rouge">scikit-learn</code></h2>

<p>Zaimplementujemy teraz nasz klasyfikator, przechodzÄ…c po kolei przez kaÅ¼dÄ… z aÅ¼ trzech potrzebnych nam metod (a w sumie to inicjalizatora i dwÃ³ch metod). Dla szczegÃ³lnie spragnionych wiedzy â€“ w <a href="https://scikit-learn.org/stable/developers/develop.html">dokumentacji</a> moÅ¼na znaleÅºÄ‡ dodatkowe informacje na temat implementacji wÅ‚asnych modeli.</p>

<h3 id="importy-i-zalÄ…Å¼ek-klasy">Importy i zalÄ…Å¼ek klasy</h3>

<p>Tworzymy wiÄ™c pusty plik <code class="highlighter-rouge">onc.py</code> i zaczynamy jak zwykÅ‚e od zaimportowania wszystkiego, co bÄ™dzie nam potrzebne podczas pracy:</p>

<ul>
  <li><code class="highlighter-rouge">numpy</code> - to juÅ¼ klasyk.</li>
  <li><code class="highlighter-rouge">BaseEstimator</code> - klasa bazowa dla wszystkich estymatorÃ³w <code class="highlighter-rouge">scikit-learn</code>, zawierajÄ…ca metody <code class="highlighter-rouge">get_param</code> i <code class="highlighter-rouge">set_params</code> (chyba nie potrzebujÄ…ce wyjaÅ›nienia).</li>
  <li><code class="highlighter-rouge">ClassifierMixin</code> - klasa zawierajÄ…ca metodÄ™ <code class="highlighter-rouge">score</code>.</li>
  <li><code class="highlighter-rouge">check_X_y</code>, <code class="highlighter-rouge">check_array</code> oraz <code class="highlighter-rouge">check_is_fitted</code> - metody sÅ‚uÅ¼Ä…ce do walidacji poprawnoÅ›ci naszych danych treningowych i testowych oraz do sprawdzenia, czy nasz model zostaÅ‚ juÅ¼ wytrenowany.</li>
  <li><code class="highlighter-rouge">DistanceMetric</code> - klasa pozwalajÄ…ca na obliczanie dystansÃ³w parowych wedle zadanej metryki (jeÅ¼eli ktoÅ› ma takÄ… potrzebÄ™, to moÅ¼na nawet zdefiniowaÄ‡ wÅ‚asnÄ… funkcjÄ™ dystansu).</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">ClassifierMixin</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.validation</span> <span class="kn">import</span> <span class="n">check_X_y</span><span class="p">,</span> <span class="n">check_array</span><span class="p">,</span> <span class="n">check_is_fitted</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">DistanceMetric</span>
</code></pre></div></div>
<p>NastÄ™pnie zaczynamy od zdefiniowania klasy naszego estymatora, ktÃ³ra dziedziczyÄ‡ bÄ™dzie po <code class="highlighter-rouge">BaseEstimator</code> oraz <code class="highlighter-rouge">ClassifierMixin</code>. Dodamy teÅ¼ komentarz, coby wiedzieÄ‡, co w ogÃ³le ta klasa implementuje.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">OptimizedNearestCentroid</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">ClassifierMixin</span><span class="p">):</span>
    <span class="s">"""
    Nearest Centroid Classifier optimized according to the three sigma rule.
    Klasyfikator najbliÅ¼szych centroidÃ³w optymalizowany reguÅ‚Ä… trzech sigm.
    """</span>
</code></pre></div></div>

<h3 id="inicjalizator-__init__">Inicjalizator (<code class="highlighter-rouge">__init__</code>)</h3>

<p>Czas zastanowiÄ‡ siÄ™, jakie argumenty bÄ™dÄ… potrzebne naszemu klasyfikatorowi. Moim zdaniem bÄ™dÄ… to:</p>

<ul>
  <li><code class="highlighter-rouge">metric</code> - nazwa metryki dystansu, z ktÃ³rej bÄ™dzie korzystaÅ‚ estymator.</li>
  <li><code class="highlighter-rouge">optimize</code> - mÃ³wiÄ…cy nam o tym, czy chcemy skorzystaÄ‡ z domyÅ›lnej wersji NC, czy moÅ¼e jednak optymalizowaÄ‡.</li>
  <li><code class="highlighter-rouge">sigma</code> - jakÄ… wielokrotnoÅ›Ä‡ odchylenia standardowego uznajemy za dopuszczalnÄ….</li>
</ul>

<p>NaleÅ¼y pamiÄ™taÄ‡ o tym, aby kaÅ¼dy z argumentÃ³w miaÅ‚ wartoÅ›Ä‡ domyÅ›lnÄ…. DziÄ™ki temu moÅ¼emy potem zainicjowaÄ‡ nasz model bez podawania argumentÃ³w.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s">'euclidean'</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metric</span> <span class="o">=</span> <span class="n">metric</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimize</span> <span class="o">=</span> <span class="n">optimize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>
</code></pre></div></div>

<h3 id="dopasowanie-modelu-do-danych-fit">Dopasowanie modelu do danych (<code class="highlighter-rouge">fit()</code>)</h3>

<p>Teraz najwaÅ¼niejsza metoda, czyli <code class="highlighter-rouge">fit()</code>. OczywiÅ›cie przekazujemy do niej nasze dane treningowe czyli <code class="highlighter-rouge">X</code> i <code class="highlighter-rouge">y</code>. NastÄ™pnie przechodzimy przez standardowÄ… procedurÄ™ sprawdzenia ksztaÅ‚tu danych, zapamiÄ™tania unikalnych klas problemu oraz przechowania <code class="highlighter-rouge">X</code> i <code class="highlighter-rouge">y</code> jako parametrÃ³w modelu:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># czy X i y maja wlasciwy ksztalt
</span>        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="c1"># przechowanie unikalnych klas problemu
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="c1"># przechowujemy X i y
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">X_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</code></pre></div></div>
<p>Teraz w koÅ„cu zrobimy coÅ›, co faktycznie jest wÅ‚aÅ›ciwe dla naszego konkretnego klasyfikatora. Inicjujemy obiekt <code class="highlighter-rouge">DistanceMetric</code> ze zdefiniowanym wczeÅ›niej dystansem <code class="highlighter-rouge">self.metric</code>, z ktÃ³rego chcemy korzystaÄ‡ przy obliczaniu odlegÅ‚oÅ›ci. Do tego przygotowujemy sobie listÄ™ <code class="highlighter-rouge">self.centroids</code>, ktÃ³ra bÄ™dzie kontenerem na wyznaczone przez nas centroidy klas i zaczynamy wczeÅ›niej opisanÄ… procedurÄ™ juÅ¼ dla kaÅ¼dej klasy osobno:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="c1"># przygotowujemy narzedzie do liczenia dystansow
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">dm_</span> <span class="o">=</span> <span class="n">DistanceMetric</span><span class="o">.</span><span class="n">get_metric</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metric</span><span class="p">)</span>
        <span class="c1"># kontener na centroidy klas
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">centroids_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># dla kazdej klasy
</span>        <span class="k">for</span> <span class="n">cl</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">:</span>
            <span class="c1"># wybieramy tylko instancje nalezace do danej klasy
</span>            <span class="n">X_class</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">y_</span> <span class="o">==</span> <span class="n">cl</span><span class="p">]</span>
</code></pre></div></div>
<p>Gdyby Å¼ycie byÅ‚o proste (<strong>a wszyscy wiemy, Å¼e nie jest</strong>), to pozostaÅ‚a czÄ™Å›Ä‡ metody <code class="highlighter-rouge">fit()</code> wyglÄ…daÅ‚aby nastÄ™pujÄ…co:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            <span class="c1"># wyliczamy centroid klasy
</span>            <span class="n">class_centroid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_class</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="c1"># dodajemy wyliczony centroid do listy
</span>            <span class="bp">self</span><span class="o">.</span><span class="n">centroids_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">class_centroid</span><span class="p">)</span>
        <span class="c1"># zwracamy klasyfikator
</span>        <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></div>
<p>I w ten sposÃ³b byÅ‚oby po sprawie.
Ale my chcemy mÃ³c skorzystaÄ‡ ze wspaniaÅ‚ej i niewÄ…tpliwie ogromnie waÅ¼nej procedury optymalizacji. W zwiÄ…zku z tym <strong>zapominamy o kodzie przedstawionym wyÅ¼ej</strong> i w jego miejsce wrzucamy kochanÄ… i caÅ‚kowicie nieodpowiedniÄ… nieskoÅ„czonÄ… pÄ™tlÄ™. W pÄ™tli zaczynamy od wyliczenia centroidu klasy a potem dodajemy moÅ¼liwoÅ›Ä‡ przerwania, jeÅ¼eli nie chcielibyÅ›my optymalizowaÄ‡ naszego modelu:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            <span class="c1"># petla
</span>            <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
                <span class="c1"># wyliczamy centroid klasy
</span>                <span class="n">class_centroid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_class</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="c1"># jeÅ¼eli nie optymalizujemy to koÅ„czymy
</span>                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimize</span> <span class="o">==</span> <span class="bp">False</span><span class="p">:</span>
                    <span class="k">break</span>
</code></pre></div></div>

<p>Przed nami wÅ‚aÅ›ciwa procedura parametryzacji. Zgodnie z zaÅ‚oÅ¼eniem liczymy odchylenie standardowe wszystkich instancji klasy (bÄ™dzie to wektor). NastÄ™pnie dodajemy pomnoÅ¼one <code class="highlighter-rouge">self.simga</code> razy odchylenie do centroidu, aby wyznaczyÄ‡ instancjÄ™ granicznÄ… (znajdujÄ…cÄ… siÄ™ 3 odchylenia standardowe od centroidu). DziÄ™ki temu moÅ¼emy z wykorzystaniem <code class="highlighter-rouge">self.dm</code> policzyÄ‡ dystans miÄ™dzy tÄ… instancjÄ… a centroidem, ktÃ³ry jednoczeÅ›nie jest najwiÄ™kszym dystansem jaki dopuszczamy dla obiektÃ³w klasy. Dalej liczymy dystans kaÅ¼dej instancji klasy od centroidu.</p>

<p>Funkcja <code class="highlighter-rouge">np.squeeze()</code> pozwala nam siÄ™ pozbyÄ‡ wszystkich wymiarÃ³w o wartoÅ›ci 1 (czyli tak naprawdÄ™ pustych), a <code class="highlighter-rouge">np.reshape()</code> jest konieczny ze wzglÄ™du na specyfikÄ™ wejÅ›Ä‡ przyjmowanych przez <code class="highlighter-rouge">DistanceMetric</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                <span class="c1"># liczymy odchylenie standardowe instancji klasy
</span>                <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X_class</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="c1"># moÅ¼liwie najdalej znajdujÄ…ca siÄ™ instancje
</span>                <span class="bp">self</span><span class="o">.</span><span class="n">borderline_</span> <span class="o">=</span> <span class="n">class_centroid</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">*</span> <span class="n">std</span><span class="p">)</span>
                <span class="c1"># maksymalny dopuszczalny dystans
</span>                <span class="n">accepted_distances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dm_</span><span class="o">.</span><span class="n">pairwise</span><span class="p">(</span>
                    <span class="n">class_centroid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">X_class</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">borderline_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">X_class</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span>
                <span class="c1"># liczymy dystanse wszystkich obiektow klasy od centroidu
</span>                <span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dm_</span><span class="o">.</span><span class="n">pairwise</span><span class="p">(</span>
                    <span class="n">class_centroid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">X_class</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">X_class</span><span class="p">))</span>
</code></pre></div></div>

<p>Teraz musimy zdecydowaÄ‡ w jaki sposÃ³b chcemy usuwaÄ‡ z klasy obserwacje odstajÄ…ce. MoglibyÅ›my zrobiÄ‡ to np. poprzez wyznaczenie indeksÃ³w instancji z dystansami wiÄ™kszymi niÅ¼ dopuszczalne (<code class="highlighter-rouge">np.argwhere</code>) i skorzystanie z funkcji <code class="highlighter-rouge">np.delete()</code>.</p>

<p>Zrobimy to jednak w inny (i czÄ™sto preferowany) sposÃ³b. Mianowicie skorzystamy z maski binarnej. Tworzymy wiÄ™c macierz <code class="highlighter-rouge">self.outliers_mask_</code> ktÃ³ra odpowiada ksztaÅ‚tem naszej macierzy <code class="highlighter-rouge">distances</code> i zawiera wartoÅ›ci <code class="highlighter-rouge">True</code> wszÄ™dzie tam, gdzie dystans przekracza dopuszczalnÄ… wartoÅ›Ä‡ (w przeciwnym razie mamy wartoÅ›Ä‡ <code class="highlighter-rouge">False</code>).</p>

<p>Teraz na zakoÅ„czenie potrzebujemy warunku, ktÃ³ry zakoÅ„czy procedurÄ™ optymalizacji jeÅ¼eli nie znajdziemy Å¼adnej obserwacji odstajÄ…cej, lub usunie z klasy znalezione <em>outliery</em>. Usuwanie odbywa siÄ™ poprzez pozostawienie w klasie tylko tych instancji, ktÃ³rych dystanse nie przekraczajÄ… wyznaczonej przez nas granicy. SposÃ³b z wykorzystaniem maski binarnej jest czÄ™sto wygodniejszy i szybszy niÅ¼ <code class="highlighter-rouge">np.delete()</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                <span class="c1"># uznajemy za outliery te instancje, ktore znajduja sie od
</span>                <span class="c1"># centroidu dalej niz 3 * std
</span>                <span class="bp">self</span><span class="o">.</span><span class="n">outliers_mask_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">distances</span> <span class="o">&gt;</span> <span class="n">accepted_distances</span><span class="p">)</span>
                <span class="c1"># konczymy optymalizacje, jezeli nie mamy outlierow
</span>                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outliers_mask_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">break</span>
                <span class="c1"># w inym przypadku pozbywamy sie outlierow
</span>                <span class="k">else</span><span class="p">:</span>
                    <span class="n">X_class</span> <span class="o">=</span> <span class="n">X_class</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">outliers_mask_</span> <span class="o">==</span> <span class="bp">False</span><span class="p">]</span>

            <span class="c1"># dodajemy wyliczony centroid do listy
</span>            <span class="bp">self</span><span class="o">.</span><span class="n">centroids_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">class_centroid</span><span class="p">)</span>
        <span class="c1"># zwracamy klasyfikator
</span>        <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></div>

<p>Kiedy juÅ¼ skoÅ„czymy optymalizacjÄ™ i wyjdziemy z pÄ™tli (lub wcale nie wykonamy optymalizacji), dodajemy wyznaczony centroid klasy do naszego kontenera i zajmujemy siÄ™ kolejnÄ… klasÄ… problemu (lub koÅ„czymy procedurÄ™ trenowania i zwracamy klasyfikator).</p>

<h3 id="predykcja-predict">Predykcja (<code class="highlighter-rouge">predict()</code>)</h3>

<p>Mamy juÅ¼ <code class="highlighter-rouge">fit()</code> wiÄ™c teraz przydaÅ‚by siÄ™ i <code class="highlighter-rouge">predict()</code>. Zaczynamy od rutyny, czyli sprawdzenia czy model zostaÅ‚ wczeÅ›niej wyuczony oraz czy dane testowe nie sÄ… puste i majÄ… odpowiedni ksztaÅ‚t.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># sprawdzenie do wywolany zostal fit
</span>        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="c1"># sprawdzenie wejscia
</span>        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></div>

<p>Dalej postÄ™pujemy wedle zasady dziaÅ‚ania przedstawionej na poczÄ…tku dzisiejszego wpisu. Wyliczamy dystanse kaÅ¼dej instancji testowej do wyznaczonych centroidÃ³w klas a nastÄ™pnie wybieramy jako predykcjÄ™ tÄ… z klas, ktÃ³rej centroidu znajdujemy siÄ™ bliÅ¼ej. PredykcjÄ™ oczywiÅ›cie zwracamy.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="c1"># liczymy dystanse instancji testowych od centroidow
</span>        <span class="n">distance_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dm_</span><span class="o">.</span><span class="n">pairwise</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">centroids_</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
        <span class="c1"># uznajemy, ze instancje naleza do klasy,
</span>        <span class="c1"># ktorej centroid znajduje sie blizej
</span>        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">distance_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># zwracamy predykcje
</span>        <span class="k">return</span> <span class="n">y_pred</span>
</code></pre></div></div>

<p>I to by byÅ‚o na tyle.</p>

<h2 id="przetestujmy-nasz-klasyfikator">Przetestujmy nasz klasyfikator</h2>

<p>WypadaÅ‚oby przetestowaÄ‡ zaimplementowany przez nas model i sprawdziÄ‡, czy i jak dziaÅ‚a. W tym celu przygotujemy sobie <strong>bardzo proste</strong> Å›rodowisko testowe. Nie dzieje siÄ™ tutaj nic, z czym nie bylibyÅ›my juÅ¼ zaznajomieni. JedynÄ… <em>nowoÅ›ciÄ…</em> moÅ¼e byÄ‡ zaimportowanie naszego modelu z wczeÅ›niej przygotowanego przez nas pliku z implementacjÄ….</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">onc</span> <span class="kn">import</span> <span class="n">OptimizedNearestCentroid</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">700</span><span class="p">,</span>
    <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">n_repeated</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">flip_y</span><span class="o">=</span><span class="mf">.15</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">32456</span><span class="p">,</span>
    <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
    <span class="n">test_size</span><span class="o">=</span><span class="mf">.3</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">OptimizedNearestCentroid</span><span class="p">(</span><span class="n">optimize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">clfo</span> <span class="o">=</span> <span class="n">OptimizedNearestCentroid</span><span class="p">()</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">clfo</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">predo</span> <span class="o">=</span> <span class="n">clfo</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Zwykly:         </span><span class="si">%.3</span><span class="s">f"</span> <span class="o">%</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Optymalizowany: </span><span class="si">%.3</span><span class="s">f"</span> <span class="o">%</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predo</span><span class="p">))</span>
</code></pre></div></div>
<p>SpÃ³jrzmy na wyniki:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt; Zwykly:         0.914
&gt;&gt; Optymalizowany: 0.924
</code></pre></div></div>
<p>Faktycznie wyglÄ…da na to, Å¼e nasza optymalizacja <em>coÅ› robi</em>. OczywiÅ›cie jak wiemy, na podstawie tak nienachalnego umysÅ‚owo eksperymentu nie moÅ¼emy wysnuÄ‡ Å¼adnych wnioskÃ³w, ale teÅ¼ nie to byÅ‚o dzisiaj naszym celem.</p>

<p>Warto za to spojrzeÄ‡ na to, jak faktycznie zachowuje siÄ™ nasz model. W tym celu rzuÄ‡my okiem na proste wykresy przestrzeni cech, obrazujÄ…ce dziaÅ‚anie optymalizacji.</p>

<table><tr>
<td> <img src="/examples/kod6/jeden.png" style="width: 310;" /> </td>
<td> <img src="/examples/kod6/dwa.png" style="width: 310;" /> </td>
<td> <img src="/examples/kod6/trzy.png" style="width: 310;" /> </td>
</tr></table>

<p>Na pierwszym od lewej wykresie widzimy przestrzeÅ„ cech, nie dzieje siÄ™ tutaj jeszcze nic niesamowitego. Wykres Å›rodkowy ukazuje wstÄ™pnie wyliczone centroidy klas (oznaczone kolorem czarnym). MagiÄ™ wÅ‚aÅ›ciwÄ… moÅ¼emy zaobserwowaÄ‡ na trzecim wykresie â€“ widzimy na nim instancje uznane za obserwacje odstajÄ…ce klasy niebieskiej (oznaczone kolorem szarym) oraz to, jak wraz z ich stopniowym usuwaniem centroid tej klasy przesuwa siÄ™ na <em>poÅ‚udniowy-wschÃ³d</em>.</p>

<p>To wszystko na dzisiaj, a za tydzieÅ„ implementacja wÅ‚asnych klasyfikatorÃ³w zespoÅ‚owych.</p>

<p><img src="/examples/kod6/dogconstr.png" alt="" /></p>
:ET