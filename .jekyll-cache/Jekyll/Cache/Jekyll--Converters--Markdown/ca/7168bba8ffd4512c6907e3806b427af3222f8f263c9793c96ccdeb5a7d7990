I"¸}<p>Kolejna Å›roda nadeszÅ‚a, a to oznacza nowÄ… porcjÄ™ przykÅ‚adÃ³w. Dzisiaj poznamy rÃ³Å¼ne sposoby walidacji krzyÅ¼owej. Nie przedÅ‚uÅ¼ajÄ…c wstÄ™pu zapraszam do lektury.</p>

<!--more-->

<h2 id="prosta-walidacja">Prosta walidacja</h2>

<h3 id="dane">Dane</h3>

<p>Z poprzednich przykÅ‚adÃ³w na pewno juÅ¼ wiemy jak wygenerowaÄ‡ lub wczytaÄ‡ dane. Dla szybkiego przypomnienia poniÅ¼ej fragment kodu generowania danych. Wprowadzona jest drobna zmiana i dodajemy parametr <code class="highlighter-rouge">weights</code>, o ktÃ³rym dowiemy siÄ™ wiÄ™cej w dalszej czÄ™Å›ci.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span>
    <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.2</span><span class="p">],</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">n_informative</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_repeated</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">flip_y</span><span class="o">=</span><span class="mf">.05</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">,</span>
    <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>
</code></pre></div></div>

<h3 id="podziaÅ‚-danych">PodziaÅ‚ danych</h3>

<p>Wykonanie walidacji modelu wymaga od nas podziaÅ‚u danych na <em>zbiÃ³r uczÄ…cy</em> i <em>zbiÃ³r testowy</em>. Trenowanie i testowanie modelu na takich samym danych jest bez sensu, a wyniki uzyskane w ten sposÃ³b nie pozwolÄ… nam okreÅ›liÄ‡ jakoÅ›ci naszego klasyfikatora. Najprostszym przykÅ‚adem jest skorzystanie z funkcji <code class="highlighter-rouge">train_test_split()</code>, ktÃ³ra pozwoli na wykonanie pojedynczego podziaÅ‚u naszego zbioru danych. Parametr <code class="highlighter-rouge">test_size</code> ustawiony na wartoÅ›Ä‡ <code class="highlighter-rouge">.30</code> oznacza, Å¼e 30% losowo wybranych prÃ³bek ze zbioru zostanie przydzielona do  <em>zbioru testowego</em> , a 70% do  <em>zbioru uczÄ…cego</em>. PamiÄ™tamy o <code class="highlighter-rouge">random_state</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
    <span class="n">test_size</span><span class="o">=</span><span class="mf">.30</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span>
<span class="p">)</span>
</code></pre></div></div>

<h3 id="budowa--i-testowanie-modelu">Budowa  i testowanie modelu</h3>

<p>PosiadajÄ…c <em>zbiÃ³r uczÄ…cy</em> moÅ¼emy zbudowaÄ‡ model. Klasyfikator wykorzystany do tego to znany z poprzedniego przykÅ‚adu <em>Naiwny klasyfikator bayesowski</em> oparty na rozkÅ‚adzie normalnym zwanym teÅ¼ rozkÅ‚adem Gaussa. Zrobimy to za pomocÄ… metody <code class="highlighter-rouge">fit()</code>, ktÃ³ra jako argumenty przyjmuje atrybuty <code class="highlighter-rouge">X_train</code> i etykiety <code class="highlighter-rouge">y_train</code> zbioru uczÄ…cego. Kolejnym krokiem jest dokonanie predykcji na <em>zbiorze testowym</em>. Wykonuje siÄ™ to za pomocÄ… metody <code class="highlighter-rouge">predict()</code>, ktÃ³ra jako argument przyjmuje atrybuty uczÄ…cego zbioru danych <code class="highlighter-rouge">X_test</code>. Zwracany jest zbiÃ³r przewidywanych etykiet <code class="highlighter-rouge">predict</code> , ktÃ³ry jest odpowiedziÄ… naszego wczeÅ›niej wyuczonego modelu. Ostatnim etapem jest wyliczenie jakoÅ›ci na podstawie wybranej metryki, w tym wypadku dokÅ‚adnoÅ›Ä‡ <code class="highlighter-rouge">accuracy_score</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">predict</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predict</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Accuracy score:</span><span class="se">\n</span><span class="si">%.2</span><span class="s">f"</span> <span class="o">%</span> <span class="n">score</span><span class="p">)</span>
</code></pre></div></div>

<p>JakoÅ›Ä‡ jakÄ… otrzymaliÅ›my jest doÅ›Ä‡ wysoka, ale nie oznacza to, Å¼e jest to wynik nas satysfakcjonujÄ…cy.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy score: 0.900
</code></pre></div></div>

<h2 id="walidacja-krzyÅ¼owa">Walidacja krzyÅ¼owa</h2>

<h3 id="k-krotna-walidacja-krzyÅ¼owa">K-krotna walidacja krzyÅ¼owa</h3>

<p>Przechodzimy teraz do duÅ¼o lepszej metody walidacji - k-krotnej walidacji krzyÅ¼owej. Pozwoli to nam na wyznaczenie jakoÅ›ci modelu w najczÄ™Å›ciej stosowany sposÃ³b. GÅ‚Ã³wnÄ… ideÄ… jest podziaÅ‚ naszego zbioru na <em>K</em> rÃ³wnych czÄ™Å›ci. NastÄ™pnie kolejno kaÅ¼da z tych czÄ™Å›ci uÅ¼ywana jest jako <em>zbiÃ³r testowy</em>, a pozostaÅ‚e jako <em>zbiÃ³r uczÄ…cy</em>. Proces ten wykonywany jest <em>K</em> razy i za kaÅ¼dym razem budujemy nowy model od nowa. NastÄ™pnie uzyskane rezultaty (np. dokÅ‚adnoÅ›Ä‡) sÄ… uÅ›rednianie lub w jakiÅ› inny sposÃ³b agregowane w jeden wynik. PoniÅ¼ej wizualizacja podziaÅ‚u dla 5-krotnej walidacji krzyÅ¼owej. CaÅ‚oÅ›Ä‡ to w pewnym uproszczeniu nasz zbiÃ³r danych podzielony na 5 losowych czÄ™Å›ci. KaÅ¼dy zielony kwadracik oznacza czÄ™Å›Ä‡ wybranÄ… jako <em>podzbiÃ³r testowy</em> , a Å¼Ã³Å‚te oznaczajÄ… <em>podzbiÃ³r uczÄ…cy</em>. W sumie mamy 5 iteracji.</p>

<p><img src="/examples/5kfold.png" alt="5kfold" /></p>

<p>W praktyce wyglÄ…da to w nastÄ™pujÄ…cy sposÃ³b. Najpierw po zaimportowaniu odpowiednich skÅ‚adnikÃ³w tworzymy obiekt <code class="highlighter-rouge">kf</code> klasy <code class="highlighter-rouge">KFold</code> z ustalonym parametrem liczby czÄ™Å›ci <code class="highlighter-rouge">n_splits</code>. W tym wypadku ustalamy wartoÅ›Ä‡ 5. PamiÄ™tajmy jeszcze o parametrze <code class="highlighter-rouge">shuffle</code> ustawionym na wartoÅ›Ä‡ <code class="highlighter-rouge">True</code> co oznacza, Å¼e dane zostanÄ… przetasowane przed podziaÅ‚em oraz o ustawieniu parametru <code class="highlighter-rouge">random_state</code>. NastÄ™pnie tworzymy sobie pustÄ… listÄ™ <code class="highlighter-rouge">scores</code>, na ktÃ³rej bÄ™dziemy pÃ³Åºniej umieszczaÄ‡ uzyskanÄ… dokÅ‚adnoÅ›Ä‡ na poszczegÃ³lnych <em>podzbiorach testowych</em>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>
<span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></div>

<p>W kolejnym kroku wykorzystujemy metodÄ™ <code class="highlighter-rouge">split</code> z argumentem <code class="highlighter-rouge">X</code>, ktÃ³ra zwrÃ³ci nam numery indeksÃ³w, prÃ³bek wybranych i podzielonych na <em>podzbiory uczÄ…ce</em> oraz <em>podzbiory testowe</em>. Odpowiednio <code class="highlighter-rouge">train_index</code> i <code class="highlighter-rouge">test_index</code>. Mechanizm pÄ™tli pozwala nam na iteracyjne wybieranie poszczegÃ³lnych podziaÅ‚Ã³w. WewnÄ…trz pÄ™tli dokonujemy juÅ¼ faktycznego podziaÅ‚u na atrybuty uczÄ…ce <code class="highlighter-rouge">X_train = X[train_index]</code> i testowe <code class="highlighter-rouge">X-test = X[test_index]</code>. To samo dotyczy etykiet <code class="highlighter-rouge">y_train, y_test = y[train_index], y[test_index]</code>. MajÄ…c juÅ¼ tak podzielony zbiÃ³r danych moÅ¼emy zbudowaÄ‡ model, dokonaÄ‡ predykcji i obliczyÄ‡ dokÅ‚adnoÅ›Ä‡ klasyfikacji, a uzyskany wynik zamieÅ›ciÄ‡ na wczeÅ›niej przygotowanej liÅ›cie <code class="highlighter-rouge">scores</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">predict</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predict</span><span class="p">))</span>
</code></pre></div></div>

<p>W ostatnim kroku uÅ›redniamy <code class="highlighter-rouge">np.mean()</code> uzyskane wyniki oraz liczymy standardowe odchylenie <code class="highlighter-rouge">np.std()</code>. Dokonujemy tego za pomocÄ… funkcji zawartych w bibliotece <code class="highlighter-rouge">numpy</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="n">std_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Accuracy score: </span><span class="si">%.3</span><span class="s">f (</span><span class="si">%.3</span><span class="s">f)"</span> <span class="o">%</span> <span class="p">(</span><span class="n">mean_score</span><span class="p">,</span> <span class="n">std_score</span><span class="p">))</span>
</code></pre></div></div>

<p>Uzyskany przez nas wynik to:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy score: 0.880 (0.051)
</code></pre></div></div>

<p><img src="/examples/cv.jpeg" alt="cv" /></p>

<ul>
  <li>
    <p>Czy to oznacza, Å¼e pogorszyliÅ›my jakoÅ›Ä‡ naszego modelu?</p>
  </li>
  <li>
    <p>Stanowcze nie!</p>
  </li>
</ul>

<p>Na pewno poprawiliÅ›my jakoÅ›Ä‡ naszego badania. MoÅ¼e siÄ™ teÅ¼ zdarzyÄ‡, Å¼e zmierzona dokÅ‚adnoÅ›Ä‡ ulegnie zmianie, ale nie zawsze bÄ™dzie siÄ™ pogarszaÄ‡.</p>

<h3 id="stratyfikowana-walidacja-krzyÅ¼owa">Stratyfikowana walidacja krzyÅ¼owa</h3>

<p>Niejednokrotnie moÅ¼emy siÄ™ spotkaÄ‡ z danymi gdzie liczba obiektÃ³w danej klasy jest liczniejsza od pozostaÅ‚ych klas. W przypadku niewielkich rÃ³Å¼nic nie ma to duÅ¼ego wpÅ‚ywu na wynik i przeprowadzanie badania. Jednak duÅ¼e rÃ³Å¼nice stajÄ… siÄ™ pewnym utrudnieniem. Nazywane jest to <em>niezbalansowaniem danych</em>. WiÄ…Å¼e siÄ™ to z zastosowaniem odpowiedniego podejÅ›cia podczas klasyfikacji i przeprowadzania eksperymentÃ³w. Za pomocÄ… prostej komendy moÅ¼emy wypisaÄ‡ sobie wszystkie etykiety:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>Co powoduje wypisanie:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0
 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0]
</code></pre></div></div>

<p>Na pierwszy rzut oka wyglÄ…da, Å¼e jest wiÄ™cej etykiet klasy â€œ0â€ niÅ¼ klasy â€œ1â€. MoÅ¼emy sobie oszczÄ™dziÄ‡ rÄ™cznego liczenia i wykorzystaÄ‡ jednÄ… z funkcji biblioteki <code class="highlighter-rouge">numpy</code> do sprawdzenia ile dokÅ‚adnie jest poszczegÃ³lnych wartoÅ›ci.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
</code></pre></div></div>

<p>Wynikiem czego jest:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(array([0, 1]), array([76, 24]))
</code></pre></div></div>

<p>Oznacza to, Å¼e w naszym zbiorze danych klas oznaczonych etykietÄ… â€œ0â€ jest dokÅ‚adnie 76, a oznaczonych etykietÄ… â€œ1â€ jest 24. Nasze dane rÃ³wnieÅ¼ sÄ… niezbalansowane. Ten wspÃ³Å‚czynnik pomiÄ™dzy liczbÄ… etykiet klas nazywamy <em>poziomem niezbalansowania</em>. Odpowiedzialny za to jest parametr generatora <code class="highlighter-rouge">weights=[0.8,0.2]</code> , za pomocÄ… ktÃ³rego ustalamy jaki bÄ™dzie udziaÅ‚ poszczegÃ³lnych klas w generowanych danych. Walidacja krzyÅ¼owa dokonuje losowego podziaÅ‚u na pozdbiory, co moÅ¼e zaburzaÄ‡ ten rozkÅ‚ad. W tym momencie bardzo przydatna okazuje stratyfikowana walidacja krzyÅ¼owa, ktÃ³ra podczas podziaÅ‚u na podzbiory zachowuje oryginalny lub zbliÅ¼ony do oryginalnego <em>poziom niezbalansowania</em>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>
<span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">predict</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predict</span><span class="p">))</span>

<span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="n">std_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Accuracy score: </span><span class="si">%.3</span><span class="s">f (</span><span class="si">%.3</span><span class="s">f)"</span> <span class="o">%</span> <span class="p">(</span><span class="n">mean_score</span><span class="p">,</span> <span class="n">std_score</span><span class="p">))</span>
</code></pre></div></div>

<p>CaÅ‚a procedura odbywa siÄ™ w podobny sposÃ³b jak przy zwykÅ‚ej k-krotnej walidacji krzyÅ¼owej. Wykorzystujemy tylko innÄ… klasÄ™ w tym wypadku <code class="highlighter-rouge">StratifiedKFold</code> oraz w metodzie <code class="highlighter-rouge">split</code> podajemy etykiety danych <code class="highlighter-rouge">y</code>, tak Å¼eby walidator wyciÄ…gnÄ…Å‚ informacje o <em>poziomie niezbalansowania</em> klas. Na koniec sprawdzamy dokÅ‚adnoÅ›Ä‡.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Accuracy score: 0.860 (0.066)
</code></pre></div></div>

<p>Wynik niÅ¼szy, ale badania jeszcze bardziej zyskaÅ‚y na jakoÅ›ci, a my jesteÅ›my bogatsi o nowÄ… wiedzÄ™.</p>

<h3 id="wielokrotna-walidacja-krzyÅ¼owa">Wielokrotna walidacja krzyÅ¼owa</h3>

<p>Jeszcze innym podejÅ›ciem jest wielokrotna walidacja krzyÅ¼owa. GÅ‚Ã³wnie zaÅ‚oÅ¼enie tego sposobu jest takie, Å¼e dokonujemy kilku powtÃ³rzeÅ„ podziaÅ‚u na podzbiory. Innymi sÅ‚owy jest to pewne zautomatyzowanie kilku krotnego procesu walidacji krzyÅ¼owej i obliczanie uÅ›rednionej wartoÅ›ci z tych wszystkich podzbiorÃ³w. Praktycznie kod programu wyglÄ…da prawie identycznie jak w przypadku normalnej walidacji, rÃ³Å¼nica jest tylko w uÅ¼yciu innej klasy <code class="highlighter-rouge">RepeatedKFold</code> oraz ustaleniu dodatkowego parametru <code class="highlighter-rouge">n_repeats</code>. W poniÅ¼szym przykÅ‚adzie wykonamy 5-krotnÄ… walidacjÄ™ krzyÅ¼owÄ… z 10-cioma powtÃ³rzeniami. W pewnym uproszeniu moÅ¼na powiedzieÄ‡, Å¼e robimy walidacjÄ™ krzyÅ¼owÄ… 10x5 (dziesiÄ™Ä‡ na piÄ™Ä‡).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RepeatedKFold</span>
<span class="n">rkf</span> <span class="o">=</span> <span class="n">RepeatedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">rkf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">predict</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predict</span><span class="p">))</span>

<span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="n">std_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Accuracy score: </span><span class="si">%.3</span><span class="s">f (</span><span class="si">%.3</span><span class="s">f)"</span> <span class="o">%</span> <span class="p">(</span><span class="n">mean_score</span><span class="p">,</span> <span class="n">std_score</span><span class="p">))</span>
</code></pre></div></div>

<p>Istnieje rÃ³wnieÅ¼ stratyfikowana wielokrotna walidacja krzyÅ¼owa. PoniÅ¼ej przykÅ‚adowy kod programu.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RepeatedStratifiedKFold</span>
<span class="n">rskf</span> <span class="o">=</span> <span class="n">RepeatedStratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">rskf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">predict</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predict</span><span class="p">))</span>

<span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="n">std_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Accuracy score: </span><span class="si">%.3</span><span class="s">f (</span><span class="si">%.3</span><span class="s">f)"</span> <span class="o">%</span> <span class="p">(</span><span class="n">mean_score</span><span class="p">,</span> <span class="n">std_score</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="leave-one-out">Leave-one-out</h3>

<p>Ostatnim juÅ¼ dzisiaj przykÅ‚adem walidacji krzyÅ¼owej jest tak leave-one-out. Jest to bardzo szczegÃ³lna odmiana k-krotnej walidacji krzyÅ¼owej, gdzie zbiÃ³r danych jest dzielona na podzbiory, zawierajÄ…ce tylko po jednym obiekcie danych. Podobnie jak w poprzednich walidacjach krzyÅ¼owych za kaÅ¼dym razem jeden podzbiÃ³r (tutaj jeden obiekt) uÅ¼ywany jest do testowania, a pozostaÅ‚e do uczenia. Tego typu metoda stosowana jest najczÄ™Å›ciej dla maÅ‚ych zbiorÃ³w danych. Od praktycznej strony zastosowanie odbywa siÄ™ w doÅ›Ä‡ analogiczny sposÃ³b jak pozostaÅ‚e dzisiejsze przykÅ‚ady.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">LeaveOneOut</span>
<span class="n">loo</span> <span class="o">=</span> <span class="n">LeaveOneOut</span><span class="p">()</span>

<span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">loo</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">predict</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predict</span><span class="p">))</span>
<span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="n">std_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Accuracy score: </span><span class="si">%.3</span><span class="s">f (</span><span class="si">%.3</span><span class="s">f)"</span> <span class="o">%</span> <span class="p">(</span><span class="n">mean_score</span><span class="p">,</span> <span class="n">std_score</span><span class="p">))</span>
</code></pre></div></div>

<p>To by byÅ‚o dzisiaj na tyle. PeÅ‚ni nowej wiedzy moÅ¼emy teraz z czystym sumieniem odejÅ›Ä‡ od komputera i w tak piÄ™knÄ… pogodÄ™ z peÅ‚nÄ… odpowiedzialnoÅ›ciÄ… za los naszego Å›wiata posiedzieÄ‡ sobie w domu.</p>
:ET