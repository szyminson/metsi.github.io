I"u<p>Kolejna roda, a z ni kolejna dawka przykad贸w. Dzisiejszy temat to rozwinicie parowych test贸w statystycznych z poprzedniego przykadu o wiksz ilo zbior贸w danych. Opr贸cz tego znajdzie si te偶 par innych nowoci. Gorco zachcam do lektury.</p>

<!--more-->

<h2 id="eksperymenty-na-wielu-zbiorach-danych">Eksperymenty na wielu zbiorach danych</h2>

<p>Dzisiaj nauczymy si wykonywa eksperymenty nie tylko na wielu klasyfikatorach, ale r贸wnie偶 na wielu zbiorach danych. W szybki spos贸b bez rozpisywania si nad tym co ju偶 byo przeprowadzimy ca procedur krok po kroku.</p>

<h3 id="modele-klasyfikacji">Modele klasyfikacji</h3>

<p>Tym razem zaczniemy od klasyfikator贸w. Najpierw przygotowujemy sownik zawierajcy wybrane modele klasyfikacji, kt贸re zostanu偶yte do dzisiejszych eksperyment贸w.  Wybieramy trzy:</p>

<ul>
  <li>Naiwny klasyfikator bayesowski - <strong>GNB</strong></li>
  <li>Maszyn wektor贸w nonych - <strong>SVM</strong></li>
  <li>Klasyfikator k najbli偶szych ssiad贸w - <strong>kNN</strong></li>
  <li>Drzewo klasyfikacyjne i regresyjne - <strong>CART</strong></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="n">clfs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'GNB'</span><span class="p">:</span> <span class="n">GaussianNB</span><span class="p">(),</span>
    <span class="s">'SVM'</span><span class="p">:</span> <span class="n">SVC</span><span class="p">(),</span>
    <span class="s">'kNN'</span><span class="p">:</span> <span class="n">KNeighborsClassifier</span><span class="p">(),</span>
    <span class="s">'CART'</span><span class="p">:</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">),</span>
<span class="p">}</span>
</code></pre></div></div>

<h3 id="dane">Dane</h3>

<p>Teraz wybieramy dane na kt贸rych chcemy przeprowadzi eksperymenty. W tym wypadku wybierzemy 20 r贸偶nych zbior贸w danych rzeczywistych. Zestawy te mo偶na pobra w postaci plik贸w CSV z repozytorium <a href="https://github.com/w4k2/benchmark_datasets/blob/master/datasets/"><code class="highlighter-rouge">benchmark_datasets</code></a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">datasets</span> <span class="o">=</span> <span class="p">[</span><span class="s">'australian'</span><span class="p">,</span> <span class="s">'balance'</span><span class="p">,</span> <span class="s">'breastcan'</span><span class="p">,</span> <span class="s">'cryotherapy'</span><span class="p">,</span> <span class="s">'diabetes'</span><span class="p">,</span>
            <span class="s">'digit'</span><span class="p">,</span> <span class="s">'ecoli4'</span><span class="p">,</span> <span class="s">'german'</span><span class="p">,</span> <span class="s">'glass2'</span><span class="p">,</span> <span class="s">'heart'</span><span class="p">,</span> <span class="s">'ionosphere'</span><span class="p">,</span>
            <span class="s">'liver'</span><span class="p">,</span> <span class="s">'monkthree'</span><span class="p">,</span> <span class="s">'shuttle-c0-vs-c4'</span><span class="p">,</span> <span class="s">'sonar'</span><span class="p">,</span> <span class="s">'soybean'</span><span class="p">,</span>
            <span class="s">'vowel0'</span><span class="p">,</span> <span class="s">'waveform'</span><span class="p">,</span> <span class="s">'wisconsin'</span><span class="p">,</span> <span class="s">'yeast3'</span><span class="p">]</span>
</code></pre></div></div>

<h3 id="eksperyment">Eksperyment</h3>

<p>Majc ju偶 dane i wybrane metody, kt贸re chcemy przetestowa. Na pocztku musimy si do eksperyment贸w troch przygotowa. Odczytujemy liczb wybranych zbior贸w za pomoc funkcji <code class="highlighter-rouge">len()</code>. Wybieramy liczb podzia贸w oraz powt贸rze. Tworzymy bardzo dobrze ju偶 znany obiekt ze stratyfikowan wielokrotn walidacj krzy偶ow <code class="highlighter-rouge">rskf</code>. Nastpnie tworzymy zerow macierz na wyniki <code class="highlighter-rouge">scores</code>. Macierz ta posiada a偶 trzy wymiary.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RepeatedStratifiedKFold</span>

<span class="n">n_datasets</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">datasets</span><span class="p">)</span>
<span class="n">n_splits</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n_repeats</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">rskf</span> <span class="o">=</span> <span class="n">RepeatedStratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="n">n_repeats</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">clfs</span><span class="p">),</span> <span class="n">n_datasets</span><span class="p">,</span> <span class="n">n_splits</span> <span class="o">*</span> <span class="n">n_repeats</span><span class="p">))</span>
</code></pre></div></div>

<p>Kolejny krok to ju偶 przeprowadzanie eksperyment贸w na wczeniej wybranych zbiorach danych. Najpierw w ptli wczytujemy te dane. Pamitajmy o tym, 偶eby zgadzay si cie偶ki do plik贸w ze zbiorami. Nastpnie w kolejnej ptli wczytany aktualnie zbi贸r dzielimy na foldy. Klonujemy modele za pomoc funkcji <code class="highlighter-rouge">clone</code> i ka偶dy z tych modeli uczymy oraz dokonujemy predykcji na odpowiednich podzbiorach danych. Na podstawie uzyskanych wynik贸w obliczamy dokadno klasyfikacji i zapisujemy w macierzy <code class="highlighter-rouge">scores</code>. W ostatnim kroku zapisujemy wszystkie wyniki do pliku.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">clone</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="k">for</span> <span class="n">data_id</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">datasets</span><span class="p">):</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s">"datasets/</span><span class="si">%</span><span class="s">s.csv"</span> <span class="o">%</span> <span class="p">(</span><span class="n">dataset</span><span class="p">),</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">","</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">fold_id</span><span class="p">,</span> <span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rskf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">clf_id</span><span class="p">,</span> <span class="n">clf_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">clfs</span><span class="p">):</span>
            <span class="n">clf</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">clfs</span><span class="p">[</span><span class="n">clf_name</span><span class="p">])</span>
            <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">])</span>
            <span class="n">scores</span><span class="p">[</span><span class="n">clf_id</span><span class="p">,</span> <span class="n">data_id</span><span class="p">,</span> <span class="n">fold_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">'results'</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="analiza-wynik贸w">Analiza wynik贸w</h3>

<p>Majc ju偶 wyniki mo偶emy przystpi do ich analizy. Na samym pocztku musimy wczyta wczeniej zapisane wyniki. Warto zwr贸ci uwag na wymiary macierzy, kt贸ra zostaa wczytana. Mo偶na to zrobi za pomoc odpowiedniego pola <code class="highlighter-rouge">shape</code> w obiekcie typu <code class="highlighter-rouge">ndarray</code> z biblioteki <code class="highlighter-rouge">numpy</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'results.npy'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Scores:</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<p>Wypisane zostan wymiary macierzy:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Scores:
 (4, 20, 10)
</code></pre></div></div>

<p>Oznacza to, 偶e w naszej macierzy znajduj si zebrane wszystkie wyniki dla 4 klasyfikator贸w, 20 baz i 10 fold贸w. Teraz musimy te wyniki uredni po foldach. Pozwoli nam to zrobi funkcja <code class="highlighter-rouge">mean</code> z ustawieniem parametru <code class="highlighter-rouge">axis=2</code> co oznacza, 偶e uredniony zostanie 3 wymiar (liczymy od 0, 1, 2 - trzeci wymiar). Nastpnie dokonamy transpozycji tej macierzy, tak aby w kolumnach (drugi wymiar) naszej macierzy znajdoway si metody, a w wierszach zbiory danych.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mean_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Mean scores:</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">mean_scores</span><span class="p">)</span>
</code></pre></div></div>

<p>Zostanie wypisana taka tabela:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mean scores:
 [[0.78188406 0.65869565 0.70289855 0.81449275]
 [0.904      0.9048     0.8496     0.7744    ]
 [0.96267175 0.96999249 0.97290146 0.94511593]
 [0.85       0.62222222 0.71666667 0.85555556]
 [0.75004244 0.69529751 0.68164417 0.69337917]
 [1.         0.99440491 0.99213064 1.        ]
 [0.83489903 0.98511853 0.98658911 0.96870061]
 [0.7305     0.71       0.6895     0.691     ]
 [0.47895903 0.92059801 0.90869324 0.87131783]
 [0.84074074 0.65       0.65925926 0.74074074]
 [0.89030181 0.94160966 0.83468813 0.88738431]
 [0.56086957 0.69565217 0.65652174 0.62463768]
 [0.9196724  0.96390663 0.98375102 0.9729484 ]
 [0.99644734 0.99672056 0.99945355 1.        ]
 [0.67102207 0.79088269 0.7858885  0.73554007]
 [1.         0.97777778 0.94666667 1.        ]
 [0.93725837 0.96155463 0.99290366 0.98481003]
 [0.81       0.8668     0.8214     0.7521    ]
 [0.95420349 0.96566804 0.95706064 0.93846865]
 [0.30521203 0.95148444 0.94609155 0.93059082]]
</code></pre></div></div>

<h3 id="rangi">Rangi</h3>

<p>W kolejnym kroku przystpimy do wyliczenia rang na podstawie urednionych wynik贸w. Pamitajmy, 偶e im wy偶sza ranga tym metoda jest lepsza. W tym wypadku rangi przypisujemy od 1 do 4, gdzie 4 to jest liczba testowanych modeli. W przypadku remis贸w dokonujemy urednienia. Pomo偶e nam w tym funkcja <code class="highlighter-rouge">rankdata()</code> z biblioteki <code class="highlighter-rouge">numpy</code>. Na koniec uzyskane rangi zamieniamy z listy na macierz <code class="highlighter-rouge">numpy </code> i wypisujemy.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">rankdata</span>
<span class="n">ranks</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">ms</span> <span class="ow">in</span> <span class="n">mean_scores</span><span class="p">:</span>
    <span class="n">ranks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rankdata</span><span class="p">(</span><span class="n">ms</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">ranks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Ranks:</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">ranks</span><span class="p">)</span>
</code></pre></div></div>

<p>Uzyskane rangi:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Ranks:
 [[3.  1.  2.  4. ]
 [3.  4.  2.  1. ]
 [2.  3.  4.  1. ]
 [3.  1.  2.  4. ]
 [4.  3.  1.  2. ]
 [3.5 2.  1.  3.5]
 [1.  3.  4.  2. ]
 [4.  3.  1.  2. ]
 [1.  4.  3.  2. ]
 [4.  1.  2.  3. ]
 [3.  4.  1.  2. ]
 [1.  4.  3.  2. ]
 [1.  2.  4.  3. ]
 [1.  2.  3.  4. ]
 [1.  4.  3.  2. ]
 [3.5 2.  1.  3.5]
 [1.  2.  4.  3. ]
 [2.  4.  3.  1. ]
 [2.  4.  3.  1. ]
 [1.  4.  3.  2. ]]
</code></pre></div></div>

<p>Nadal taki spos贸b jest mao czytelny i trudno jest jednoznacznie okreli, kt贸ra metoda jest nalepsza. Dlatego teraz dokonujemy urednienia uzyskanych rang i wypisujemy uzyskane wyniki.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mean_ranks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ranks</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Mean ranks:</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">mean_ranks</span><span class="p">)</span>
</code></pre></div></div>

<p>Urednione rangi:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Models:  ['GNB', 'SVM', 'kNN', 'CART']
Mean ranks:  [2.25 2.85 2.5  2.4 ]
</code></pre></div></div>

<p>Na podstawie tych wynik贸w mo偶na stwierdzi, 偶e metody s na do zbli偶onym poziomie, ale nadal nie wiemy o tym, czy te wyniki s statystycznie istotne.</p>

<h3 id="testy-parowe">Testy parowe</h3>

<p>Majc ju偶 rangi mo偶emy przystpi do test贸w parowych, kt贸re pierwszy raz pojawiy si na poprzednich przykadach. Dzisiaj zamiast <em>testu T Studenta</em> zostanie u偶yty <em>test Wilcoxona</em>. U偶ycie tego testu oznacza zaimportowanie oraz wywoanie funkcji <code class="highlighter-rouge">ranksums</code> z biblioteki <code class="highlighter-rouge">scipy</code>. Ustawiamy warto <code class="highlighter-rouge">alpha</code>, kt贸ra bdzie przyjtym przez nas progiem ufnoci podczas sprawdzania istotnoci.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">ranksums</span>

<span class="n">alfa</span> <span class="o">=</span> <span class="mf">.05</span>
<span class="n">w_statistic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">clfs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">clfs</span><span class="p">)))</span>
<span class="n">p_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">clfs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">clfs</span><span class="p">)))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">clfs</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">clfs</span><span class="p">)):</span>
        <span class="n">w_statistic</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">p_value</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">ranksums</span><span class="p">(</span><span class="n">ranks</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ranks</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
</code></pre></div></div>

<p>Teraz utworzymy du偶o czytelniejsze tabele za pomoc biblioteki <code class="highlighter-rouge">tabulate</code>. Odbywa to si podobnie jak w ostatnim przykadzie. Jedyna r贸偶nica to, 偶e ustawimy dynamicznie nazwy nag贸wk贸w i nazwy kolumn z wykorzystaniem kluczy sownika modeli klasyfikacji.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tabulate</span> <span class="kn">import</span> <span class="n">tabulate</span>

<span class="n">headers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">clfs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="n">names_column</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">clfs</span><span class="o">.</span><span class="n">keys</span><span class="p">())),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">w_statistic_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">names_column</span><span class="p">,</span> <span class="n">w_statistic</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">w_statistic_table</span> <span class="o">=</span> <span class="n">tabulate</span><span class="p">(</span><span class="n">w_statistic_table</span><span class="p">,</span> <span class="n">headers</span><span class="p">,</span> <span class="n">floatfmt</span><span class="o">=</span><span class="s">".2f"</span><span class="p">)</span>
<span class="n">p_value_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">names_column</span><span class="p">,</span> <span class="n">p_value</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">p_value_table</span> <span class="o">=</span> <span class="n">tabulate</span><span class="p">(</span><span class="n">p_value_table</span><span class="p">,</span> <span class="n">headers</span><span class="p">,</span> <span class="n">floatfmt</span><span class="o">=</span><span class="s">".2f"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">w-statistic:</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">w_statistic_table</span><span class="p">,</span> <span class="s">"</span><span class="se">\n\n</span><span class="s">p-value:</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">p_value_table</span><span class="p">)</span>
</code></pre></div></div>

<p>Po wypisaniu widzimy uzyskane statystyki i wartoci p-wartoci:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>w-statistic:
         GNB    SVM    kNN    CART
----  -----  -----  -----  ------
GNB    0.00  -1.61  -0.62   -0.50
SVM    1.61   0.00   0.99    1.30

kNN    0.62  -0.99   0.00    0.28
CART   0.50  -1.30  -0.28    0.00

p-value:
         GNB    SVM    kNN    CART
----  -----  -----  -----  ------
GNB    1.00   0.11   0.53    0.62
SVM    0.11   1.00   0.32    0.19
kNN    0.53   0.32   1.00    0.78
CART   0.62   0.19   0.78    1.00
</code></pre></div></div>

<p>Nastpnie musimy wyznaczy macierz przewag. Procedura przebiega w do analogiczny spos贸b jak w ostatnich przykadach. Jedynie teraz u偶ywamy zmiennej <code class="highlighter-rouge">w_statistic</code>, do kt贸rej zostay zapisane wyniki z parowego <em>testu Wilcoxona</em>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">advantage</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">clfs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">clfs</span><span class="p">)))</span>
<span class="n">advantage</span><span class="p">[</span><span class="n">w_statistic</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">advantage_table</span> <span class="o">=</span> <span class="n">tabulate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
    <span class="p">(</span><span class="n">names_column</span><span class="p">,</span> <span class="n">advantage</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">headers</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Advantage:</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">advantage_table</span><span class="p">)</span>
</code></pre></div></div>

<p>Poni偶ej wyniki:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Advantage:
         GNB    SVM    kNN    CART
----  -----  -----  -----  ------
GNB       0      0      0       0
SVM       1      0      1       1
kNN       1      0      0       1
CART      1      0      0       0
</code></pre></div></div>

<p>Pamitajmy jednak, 偶e po pierwsze posiadamy wyznaczone rangi, a po drugie nie wiemy czy te wyniki s istotne statystycznie. Teraz mo偶emy przej do kluczowego momentu, kt贸ry pozwoli nam to okreli. Robimy to w dosownie identyczny spos贸b jak byo to pokazane w ostatnim przykadzie.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">significance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">clfs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">clfs</span><span class="p">)))</span>
<span class="n">significance</span><span class="p">[</span><span class="n">p_value</span> <span class="o">&lt;=</span> <span class="n">alfa</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">significance_table</span> <span class="o">=</span> <span class="n">tabulate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
    <span class="p">(</span><span class="n">names_column</span><span class="p">,</span> <span class="n">significance</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">headers</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Statistical significance (alpha = 0.05):</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">significance_table</span><span class="p">)</span>
</code></pre></div></div>

<p>Wypisujemy wyniki i sprawdzamy:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Statistical significance (alpha = 0.05):
         GNB    SVM    kNN    CART
----  -----  -----  -----  ------
GNB       0      0      0       0
SVM       0      0      0       0
kNN       0      0      0       0
CART      0      0      0       0
</code></pre></div></div>

<p>Niestety, w 偶adnym testowanym zestawieniu nie mo偶na odrzuci hipotezy zerowej. Oznacza to, 偶e wyniki jakie ostatecznie uzyskalimy s nieistotne statystycznie po przeprowadzeniu odpowiednich test贸w parowych. Mimo r贸偶nic jakie wystpuj pomidzy przetestowanymi modelami klasyfikacji eksperymenty przeprowadzone na wybranych przez nas zbiorach danych nie pozwalaj stwierdzi, kt贸ra z tych metod jest lepsza z istotnoci statystyczn. <strong>Jest to dla nas bardzo wa偶na lekcja m贸wica o tym,偶e lepszy nie zawsze musi oznacza lepszy statystycznie.</strong></p>

<p><img src="/examples/no-sd.jpg" alt="" /></p>
:ET